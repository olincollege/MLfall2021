
\documentclass{tufte-handout}
\usepackage{fontawesome}
\usepackage{../../CommonLatexPackages/machine_learning_preamble_1.0}

\fancypagestyle{firstpage}

{\rhead{Module 1 Project\linebreak \textit{Version: \today}}}

\title{Module 1 Project: Computer Vision Application}
\author{Machine Learning}
\date{Fall 2021}

\begin{document}

\maketitle
\thispagestyle{firstpage}

Computer vision and image classification tools are rapidly reshaping our lives. Machine learning approaches have driven recent progress in an array of technologies that have the potential to realize huge positive impacts on our world. These tools have become increasingly easy to apply to masses of existing data, however, these tools and their real-world application do not always have the results that their creators intend. To achieve better outcomes, it is important to deeply consider the potential implications and limitations throughout the application design and implementation process. This project aims to help you to begin to develop the skills to understand, implement, and critically evaluate machine learning systems. 

This document is quite long and contains links to other information and resources. The table of contents below should help you nagivate to various sections.

%\begin{learningobjectives}
%\bi
%\item Apply 
%
%\ei
%\end{learningobjectives}

\tableofcontents

\pagebreak

\section{Goal-Setting and Customization}

In the words of one of your course assistants, Jerry, your instructors define a good project as one that successfully uses a neural net on a dataset and demonstrates iteration (you try the neural net, you make improvements, you try again). If you wish to go deeper, that's great!

However, we also (informally) ask you to consider your own learning goals. Is there a specific skill you want to practice? Is there a way of learning (like, asking for help more, trying code without looking at examples first, etc) that you want to practice? We ask you to think about these, and ask your partner about their learning goals.  You can customize this project to support your own learning, and we are happy to help you shape the project to support your goals and challenge yourself. 


\section{Project Description}
You will be working in teams of two. In this project, you will contemplate a potential application for computer vision (machine learning of images) and you will implement a machine learning algorithm on a data set in support of this application. As part of this project, you will:
\bi[leftmargin=.5in]
\item Document the important considerations for your application (e.g., what data are available for training, how well would the algorithm need to work to create value, how could the algorithm and application be tested (beyond the testing you choose to implement), what are the implications of this application in the world, what are the stakeholders, what are the risks). 
\item Build and iterate on a computer vision model as a step toward this application. We don’t expect you to build the entire application, just to work toward implementing an early version of an algorithm that could be used for this application. It’s likely that you will use a convolutional neural network, but not required. Please talk with us if you plan on using a different type of model. You will build and test multiple versions of your model. You can use pre-built libraries like pytorch.
\item Visualize key aspects of your data/model. Include at least two visualizations in your report with clear labeling and a discussion of their meaning. You’ll probably find it valuable to visualize many aspects of the data and model as you work. This is helpful in sanity checking and can give good insight into how the model is working.
\item Test and evaluate your model. This will likely include accuracy on training and test sets. Depending on your application, you may also want to collect your own photos, find other images online, and/or manipulate images from your original test set (adding noise, shifting/flipping images, etc). You will also evaluate the effectiveness of this model for your specific application and describe the limitations of your current model and data.
\item Document your final analysis pipeline for transparency. (A simplified version that does not need to include every parameter, visualization, or tweak that you tried.)
\ei

\section{Timeline}

The project will officially launch on October 1, 2021 and end on October 22, 2021 (report may be due before). We will have several interim deadlines and some other activities happening during class that may require preparation. We may choose to adjust deadlines based on class progress and feedback. We will keep a \href{https://docs.google.com/spreadsheets/d/1TyzKsfdCvZEzfaYswfJSHiYH2JU73gd0KCIOwQijvFo/edit\#gid=952438279}{ tentative schedule in this spreadsheet} and use Canvas to share interim deadlines.


\section{Deliverables \& Assessment}

\subsection{Deliverables}
\be
\item A clean report in notebook (.ipynb) format with thoughtful explanations of the considerations for your computer vision application and important highlights of your analysis. This should include in-line code and figures.
This should NOT include every analysis and figure that you generated. Please choose three interesting things that you investigated and share your code and figures for these. You can mention other things that you investigated in the text and include the code in your repository. This is about quality, not quantity.

\item A pdf of your clean notebook with all of the cells run and graphs appearing clearly.

\item Your self-assessment of constructive engagement (letter grade and explanation with descriptions of your actions and/or pointers to evidence in your work). 
\ee

There are rubrics in the appendices in this document. The intention of these rubrics is to create shared expectations about the project, while also giving you lots of latitute to explore and emphasize your own learning.\\

\subsection{Assessment}

Assessment for this project will be evenly split between \textbf{constructive engagagement}, which is self-assessed by you, and \textbf{quality of project work}, which will be assessed by faculty.

\vspace{1em}

\textbf{50\% Constructive engagement (individual):} You will self-assess based on our shared rubric and include an explanation of your self-assessment. This aspect of your grade will be largely based on your self-assessment, but we may adjust your constructive engagement based on your explanation and our observations.\\
\vspace{1em}

\textbf{50\% Project work and report (team):}  We will assess and provide feedback on your final report. This should be in the form of a Jupyter notebook. This notebook should not be an extensive record of all the work you did, rather a polished report that explains the work you did with accompanying code. \\

\vspace{1em}
We want to acknowledge that we are all coming to this course with varying experience with machine learning. This is great, and we’re actively trying to help you all have positive learning experiences. We intend for the course not to require prior experience with machine learning to participate, learn new things, and feasibly receive a high grade. With this in mind, we want to be explicit about how we anticipate this playing out in assessment. \\
With the project work and report, we will assess with a mental model of a team beginning the course with no real machine learning experience. The key aspects of this assessment are described in the rubric. In practice, this means that everyone could receive an A for this part, and it will be more challenging for some people than others. \\
With constructive engagement, we are asking you to self-assess. One aspect of constructive engagement involves challenging yourself. 
We’re trying this strategy to avoid the situation where we try to estimate your previous experience and base our expectations around these assumptions. This relies on an honest self-assessment of your own constructive engagement. 

\subsection{Class-owned editable resources file}

We will continue to use Discord and office hours to answer questions. However, we also know that a great deal of learning happens outside of those places.  \href{https://docs.google.com/document/d/14C1dtiLiPF2cAy3alt4PLl0UhkuxPrDZsHgeq-pjwAU/edit?usp=sharing}{This shared, class-owned, editable document} is intended to serve as a communal set of resources for troubleshooting as we figure things out together. Anyone can add to it, so as you run into problems (you will) and figure out solutions (sometimes on your own, sometimes with help), please add to this document to help others. Learning is not a zero-sum game.

We also want to note that the field of machine learning is constantly changing, as are things like libraries and toolboxes. It's likely that you'll run into something that worked 2 years ago and does not work now. Please help us find and fix these things!



\section{Appendix A: Some questions to consider}
\href{https://www.notion.so/ANN-Project-Framing-76e1b6af347f475a983487996ac9760d}{These questions are taken from this webpage.}
\bi
\item What do you want your model to be able to do?
\item How can you imagine your model (or an extension of it) being used in the real world? Feel free to get creative.
\item If those ideas came true, who might they affect? In what ways?
\item What measures would your model's real-world implementers need to take to ensure its effects live up to your intentions?
\item What pitfalls might your model fall into, and what could you quantitatively measure to avoid those?
\item Why was the dataset you used to train your model created?
\item In what other ways could the same data be used? How do you feel about those possibilities?
\item How was that dataset assembled? From where was the data sourced? Who or what labeled it? If there are any elements of this process you think were either particularly well done or problematic, how so?
\item If your dataset contains information about people, to what degree did those people have agency over their inclusion? Do you feel that matters in this case? Why or why not?
\item Skimming through your dataset, does anything stand out to you about representation in its contents?
\item Do you feel the potential use cases for this dataset justify it being created and published? Why or why not?
\ei

\section{Appendix B: Computer Vision Datasets}
%\subsection*{PyTorch built-in datasets}\label{pytorch-built-in-datasets}
%\bi
%\tem
%\href{https://www.notion.so/Entry-Level-Computer-Vision-Datasets-cc6fb53f51324779b29e26337642a649}{Entry-Level Computer Vision Datasets}
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#torchvision.datasets.CIFAR10}{CIFAR-10}:
%  60,000 labeled 32x32 color images of 10 classes of objects
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#torchvision.datasets.CIFAR100}{CIFAR-100}:
%  60,000 labeled 32x32 color images of 100 fine classes of objects, also
%  grouped into 20 course superclasses
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#emnist}{EMNIST}:
%  800,000 labeled 28x28 grayscale images of handwritten digits and
%  letters (uppercase and lowercase) that expand the MNIST dataset
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#fashion-mnist}{Fashion-MNIST}:
%  70,000 labeled 28x28 grayscale images of 10 classes of clothing
%  articles
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#imagenet}{ImageNet
%  2012}: 1,331,167 labeled color images of tens of thousands of classes
%  of nouns in the WordNet hierarchy
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#kmnist}{KMNIST}:
%  70,000 labeled 28x28 grayscale images of 10 classes of handwritten
%  Japanese Hiragana characters
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#lsun}{LSUN}:
%  708,564 labeled large-scale color images of 10 classes of
%  scenes/settings
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#mnist}{MNIST}:
%  70,000 labeled 28x28 grayscale images of 10 classes of handwritten
%  digits
%\item
%  \href{https://pytorch.org/docs/stable/torchvision/datasets.html\#svhn}{SVHN}:
%  Color images of house numbers (addresses) obtained from Google Street
%  View w/ labeled bounding boxes around individual digits
%\ei

\subsection{TensorFlow built-in}\label{tensorflow-built-in-datasets}

There are numerous datasets for Image Classification on the \href{https://www.tensorflow.org/datasets/catalog/beans}{Tensorflow datasets} documentation page. We strongly suggest choosing one from the Image Classification category (see tab on left) instead of the other categories, unless you have substantial prior machine learning experience. 

\bi
\item
Cats and Dogs: 25,000 labeled images of cats and dogs
\item
CelebA: 202,599 total face images of 10,177 identities w/ facial landmarks,
  aligned \& cropped images, bounding boxes, and binary attribute labels
\item
  CIFAR-10:
  60,000 labeled 32x32 color images of 10 classes of objects
\item
CIFAR-100:
  60,000 labeled 32x32 color images of 100 fine classes, also grouped
  into 20 course superclasses
\item
 CIFAR-10-C:
  Images from CIFAR-10 manipulated using 15 common corruptions at 5
  levels of severity each
\item
Colorectal Histology: 5,000 labeled 150x150 color histological images of 8
  tissue types of human colorectal cancer
\item
CBIS-DDSM:
  2,620 labeled scanned film mammography studies of normal, benign, and
  malignant cases of human breast cancer
\item
 Diabetic
  Retinopathy Detection: 88,712 labeled images of eyes with diabetic
  retinopathy at none, mild, moderate, severe, and proliferative levels
  of severity
\item
EMNIST:
  800,000 labeled 28x28 grayscale images of handwritten digits and
  letters (uppercase and lowercase) that expand the MNIST dataset
\item
  Fashion-MNIST:
  70,000 labeled 28x28 grayscale images of 10 classes of clothing
  articles
\item
 Horses
  or Humans: 1,283 labeled 300x300 color images of cgi humans and
  horses
\item
ImageNet
  2012: 1,331,167 labeled color images of tens of thousands of classes
  of nouns in the WordNet hierarchy
\item
  ImageNet-C
  2012: Images from ImageNet 2012 manipulated using 12 common
  corruptions at 5 levels of severity each
\item
 KMNIST:
  70,000 labeled 28x28 grayscale images of 10 classes of handwritten
  Japanese Hiragana characters
\item
  LSUN:
  708,564 labeled large-scale color images of 10 classes of
  scenes/settings
\item
 MNIST:
  70,000 labeled 28x28 grayscale images of 10 classes of handwritten
  digits
\item
 MNIST-C:
  Images from MNIST manipulated using 15 common corruptions at 5 levels
  of severity each
\item
  Omniglot:
  38,300 labeled grayscale images of 1,623 classes of handwritten
  characters from 50 alphabets w/ stroke data in {[}x,y,t{]} coordinates
  where time (t) is in milliseconds
\item
 Oxford
  Flowers: 8,189 labeled color images of 102 classes of flowers
\item
  Oxford-IIIT
  Pet: 7,349 labeled color images of 37 classes of pet breeds w/ tight
  bounding boxes around the heads and pixel-level
  foreground-background-boundary segmentation
\item
  PatchCamelyon:
  327,680 labeled 96x96 color images of histopathologic lymph node scans
  where metastatic tissue is either present or absent
\item
PetFinder:
  72,776 color images of dogs and cats w/ many attribute labels
\item
  Quick,
  Draw! Bitmap: 50,426,266 labeled 28x28 grayscale images of 345
  classes of drawn objects contributed by Quick, Draw! players
\item
NWPU-RESISC45:
  31,500 labeled 256x256 color images of 45 classes of scenes/settings
\item
  Rock
  Paper Scissors: 2,892 labeled 300x300 color images of cgi hands in
  rock, paper, and scissors gestures
\item
small
  NORB: 48,600 labeled 96x96 color images of 5 classes of toys
\item
 Sun397:
  108,753 labeled color images of 397 classes of scene/settings
\item
 SVHN-cropped:
  600,000 labeled 32x32 color images of digits of house numbers
  (addresses) obtained from Google Street View
\item
TF
  Flowers: 3,670 labeled color images of 5 classes of flowers
\item
 UC
  Merced Land Use: 2,100 labeled 256x256 color aerial images of 21
  classes of land uses collected from USGS National Map Urban Area
  Imagery
\ei

\subsection{Other datasets}\label{other-datasets}

\bi
\item
  \href{http://eidolon.univ-lyon2.fr/~remi1/Bark-101/}{Bark-101}: 2,594
  labeled color images of 101 classes of tree bark
\item
  \href{http://www.vision.caltech.edu/Image_Datasets/Caltech101/}{Caltech
  101}: 9,146 labeled color images of 101 classes of objects
\item
  \href{http://www.vision.caltech.edu/Image_Datasets/Caltech256/}{Caltech
  256}: 30,607 labeled color images of 256 classes of objects
%\item
 % \href{https://dataturks.com/projects/dominique.paul.info/cars2}{CARS}:
 % 604 labeled color images with either cars or no cars
%\item
 % \href{http://www.jdl.ac.cn/peal/index.html}{CAS-PEAL-R1}: 30,900
 % labeled grayscale images of 1,040 faces (exclusively Chinese)
\item
  \href{https://github.com/detectRecog/CCPD}{CCPD}: 300,000 labeled
  color images of license plates w/ bounding boxes, license plate
  numbers, and several dimension labels
\item
  \href{https://github.com/BayesWatch/cinic-10}{CINIC-10}: A drop-in
  replacement for CIFAR-10 with 270,000 images collected by downsampling
  images from ImageNet
%\item
 % \href{https://dataturks.com/projects/miaozh17/Crack\%20Classification}{CRACK}:
%  1,428 labeled 299x299 color images with either cracks or no cracks
%\item
 % \href{https://cyberextruder.com/face-matching-data-set-download/}{CyberExtruder
%  Ultimate Face Matching Dataset}: 10,205 labeled 600x600 color images
 % of 1000 faces scraped from the internet
\item
  \href{http://iab-rubric.org/resources/dfw.html}{DFW}: 11,157 labeled
  color images of 1000 subjects' faces, including examples of that
  subject attempting to obfuscate their face and of other individuals
  attempting to impersonate that subject
%\item
 % \href{https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces}{DiF}:
%  1,000,000 color images of diverse faces w/ dimension labels for
 % objective physical properties
\item
  \href{https://www.nist.gov/itl/iad/image-group/color-feret-database}{FERET}:
  14,126 labeled 384x286 color images of 1,119 faces
%\item
 % \href{https://www.vision.ee.ethz.ch/datasets_extra/food-101/}{Food-101}:
 % 101,000 labeled 512x512 color images of 101 classes of food
\item
  \href{http://www.ivl.disco.unimib.it/activities/food475db/}{Food-475}:
  247,636 labeled color images of 475 classes of food
\item
  \href{http://www.ivl.disco.unimib.it/activities/food524db/}{Food-524}:
  247,636 labeled color images of 524 classes of food
\item
  \href{http://eidolon.univ-lyon2.fr/~remi1/HistAerialDataset/}{HistAerial}:
  4,900,000 labeled grayscale aerial images of 7 classes of ground use
\item
  \href{https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/}{IMDb}:
  460,723 color images of celebrities w/ bounding boxes around faces and
  attribute labels including DOB, name, binary gender (literally)
\item
  \href{https://github.com/xpwu95/IP102}{IP102}: 75,000 labeled color
  images of 102 classes of insects
\item
  \href{https://github.com/smilell/AG-CNN}{LAG}: 11,760 labeled color
  images of suspicious or negative glaucoma samples
\item
  \href{http://vis-www.cs.umass.edu/lfw/index.html}{LFW}: 13,233 labeled
  250x250 color images of 5,749 faces
\item
  \href{https://talhassner.github.io/home/projects/lfwa/index.html}{LFW-a}:
  Images from LFW that have been aligned using a commercial face
  alignment software
\item
  \href{http://conradsanderson.id.au/lfwcrop/}{LFWcrop}: Images from LFW
  that have been cropped to only include the contents of facial bounding
  boxes
\item
  \href{https://github.com/Tencent/tencent-ml-images}{ML-Images}:
  17,698,491 labeled color images of 11,166 classes of objects
\item
  \href{http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html}{Multi-PIE}:
  750,000 labeled color images of 337 faces
\item
  \href{https://www.ri.cmu.edu/project/pie-database/}{PIE}: 41,368
  labeled color images of 68 faces
\item
  \href{https://github.com/facebookresearch/qmnist}{QMNIST}: An
  extension of the MNIST data set to include 50,000 additional test
  images
%\item
 % \href{https://stevewongv.github.io/derain-project.html}{Real Rain}:
 % 29,500 labeled color image pairs, one with and one without rain
%\item
 % \href{https://dataturks.com/projects/sheerun/rooms}{ROOMS}: 10,029
 % labeled (out of 20,001 total) color images of 6 classes of room types
\item
  \href{http://www.scface.org/}{SCface}: 4,160 labeled color images of
  130 faces taken from 6 security cameras of varying quality in either
  visible light or infrared mode
\item
  \href{https://sites.google.com/view/sof-dataset}{SoF}: 42,592 labeled
  color images of 112 faces (exclusively glasses-wearing)
\item
  \href{http://vision.stanford.edu/aditya86/ImageNetDogs/}{Stanford
  Dogs}: 20,580 labeled color images of 120 breeds of dogs w/ bounding
  boxes
\item
  \href{https://github.com/PKU-IMRE/VERI-Wild}{VERI-Wild}: 416,314
  labeled color images of 40,671 vehicles
\ei


\section{Appendix C: Rubrics}

\subsection{Constructive engagement}
You will self-assess your constructive engagement using the rubric \href{https://docs.google.com/spreadsheets/d/137LKE5SBLGwmQfQOFcwmIesLeGqv5UYEtp9o0Jw573o/edit?usp=sharing}{here}. This rubric was designed in collaboration with the students from the first iteration of this course. Instead of re-inventing the wheel, we will discuss this rubric as a class and make revisions as appropriate.


\subsection{Jupyter Notebook}
Please submit both a pdf of your Juptyer Notebook (with the code executed) and an executable Jupyter Notebook to Canvas. 

We suggest you work in the Google CoLab environment (like you have for the assignments). When you are ready to submit, please download your notebook as an ipynb file, and submit that to Canvas. 

Your project will be graded on the following aspects. %Each item is worth 3 points.\\
\vspace{1em}
\textbf{General}
\begin{enumerate}[leftmargin=.5in]
\item The notebook is well-organized.
\item The notebook balances code and text well.
\item The notebook runs without errors in the Google CoLab environment.
\item The notebook is free of typos.
\item The notebook demonstrates good coding practices, including but not limited to docstrings, appropriate variable names, and comments as appropriate.
\item The code demonstrates a significant amount of effort (roughly 3 assignments'
  worth), made clear by careful consideration of the data, model, and implications for the model you developed.
\end{enumerate}

\textbf{Motivation}
\begin{enumerate}[resume, leftmargin=.5in]
\item It is clear what data you used.
\item The application of your algorithm is clear, and well-motivated.
\item The notebook explains how well the algorithm would need to work to provide value
\item The notebook explains how the algorithm is to be evaluated (how do you know it is working well?)
\item The notebook explains the implications of this approach (including risks and stakeholders)
\end{enumerate}

\textbf{Implementation}
\begin{enumerate}[resume, leftmargin=.5in]
\item The notebook explains the dataset and the features, in text and/or graphics.
\item The notebook contains a computer vision model.
\item The computer vision model is implemented correctly.
\item There is evidence that iteration has occurred in the model (this can be in the text).
\end{enumerate}

\textbf{Conclusion}
\begin{enumerate}[resume, leftmargin=.5in]
\item The notebook accurately describes the effectiveness of the model in text.
\item The notebook describes the limitations of the model.
\item The code generates two visualizations of the data.
\item The visualizations have clear labels and connections to the text.
\end{enumerate}


\end{document}
